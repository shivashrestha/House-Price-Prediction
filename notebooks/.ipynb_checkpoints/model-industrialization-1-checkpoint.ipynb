{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c78a539e-e393-4f68-af45-eff71659d72f",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f51f4-d968-40da-9c56-517b51c92d19",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab1a9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "FILE_PATH = \"../data/train.csv\"\n",
    "dataset = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# ************Features Selection************\n",
    "\n",
    "# Continuous features\n",
    "continuous_features = ['LotArea', 'YearBuilt', '1stFlrSF', 'GrLivArea']\n",
    "\n",
    "# Categorical features\n",
    "kitchen_quality_column = 'KitchenQual'\n",
    "categorical_features = ['Neighborhood', 'HouseStyle', 'OverallQual', 'OverallCond', kitchen_quality_column]\n",
    "kitchen_quality_dict = {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0}\n",
    "\n",
    "# Target value\n",
    "targeted = 'SalePrice'\n",
    "\n",
    "# ************Preprocessing*****************\n",
    "\n",
    "\n",
    "def preprocess_data(X, fit=False):\n",
    "\n",
    "    # Initialize the scalers and encoders\n",
    "    scaler = StandardScaler()\n",
    "    one_hot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "    \n",
    "    # Initialize the imputers\n",
    "    numeric_imputer = SimpleImputer(strategy='median')\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    if(fit)\n",
    "        # Fit and transform continuous features for training set\n",
    "        X_train_continuous = X_train[continuous_features]\n",
    "        numeric_imputer.fit(X_train_continuous)\n",
    "        X_train_continuous = numeric_imputer.transform(X_train_continuous)\n",
    "        scaler.fit(X_train_continuous)\n",
    "        X_train_continuous = scaler.transform(X_train_continuous)\n",
    "        \n",
    "        # Fit and transform categorical features for training set\n",
    "        X_train_categorical = X_train[categorical_features].copy()\n",
    "        X_train_categorical[kitchen_quality_column] = X_train_categorical[kitchen_quality_column].map(kitchen_quality_dict)\n",
    "        categorical_imputer.fit(X_train_categorical)\n",
    "        X_train_categorical = categorical_imputer.transform(X_train_categorical)\n",
    "        one_hot_encoder.fit(X_train_categorical[:, :-1])\n",
    "        X_train_categorical_encoded = one_hot_encoder.transform(X_train_categorical[:, :-1])\n",
    "        X_train_kitchen_quality = X_train_categorical[:, -1].reshape(-1, 1)\n",
    "        # Combine preprocessed features for training set\n",
    "        X_train_processed = np.hstack((X_train_continuous, X_train_categorical_encoded, X_train_kitchen_quality))\n",
    "        # Save the model, encoders, and scalers\n",
    "        joblib.dump(scaler, \"../models/scaler.joblib\")\n",
    "        joblib.dump(one_hot_encoder, \"../models/one_hot_encoder.joblib\")\n",
    "        joblib.dump(numeric_imputer, \"../models/numeric_imputer.joblib\")\n",
    "        joblib.dump(categorical_imputer, \"../models/categorical_imputer.joblib\")\n",
    "    else:\n",
    "        # Load preprocessors\n",
    "        scaler = joblib.load(\"models/scaler.joblib\")\n",
    "        one_hot_encoder = joblib.load(\"models/one_hot_encoder.joblib\")\n",
    "        numeric_imputer = joblib.load(\"models/numeric_imputer.joblib\")\n",
    "        categorical_imputer = joblib.load(\"models/categorical_imputer.joblib\")\n",
    "        \n",
    "        # Transform continuous features\n",
    "        X_continuous = X[continuous_features]\n",
    "        X_continuous = numeric_imputer.transform(X_continuous)\n",
    "        X_continuous = scaler.transform(X_continuous)\n",
    "        \n",
    "        # Transform categorical features\n",
    "        X_categorical = X[categorical_features].copy()\n",
    "        X_categorical[kitchen_quality_column] = X_categorical[kitchen_quality_column].map(kitchen_quality_dict)\n",
    "        X_categorical = categorical_imputer.transform(X_categorical)\n",
    "        X_categorical_encoded = one_hot_encoder.transform(X_categorical[:, :-1])\n",
    "        X_kitchen_quality = X_categorical[:, -1].reshape(-1, 1)\n",
    "        # Combine preprocessed features\n",
    "        X_processed = np.hstack((X_continuous, X_categorical_encoded, X_kitchen_quality))\n",
    "    \n",
    "    \n",
    "    return  X_train_processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f42f7",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3110b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(data: pd.DataFrame) -> dict[str, str]:\n",
    "    # Split the data into train and test sets\n",
    "    train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Split features and target from the train set\n",
    "    X_train = train_set.drop(columns=[targeted])\n",
    "    y_train = train_set[targeted]\n",
    "        \n",
    "    # Preprocess training data\n",
    "    X_train_processed = preprocess_data(X_train)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_processed, y_train)\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(model, \"../models/model.joblib\")\n",
    "\n",
    "    # Split features and target from the test set\n",
    "    X_test = test_set.drop(columns=[targeted])\n",
    "    y_test = test_set[targeted]\n",
    "\n",
    "    # Model evaluation\n",
    "    X_test = test_set.drop(columns=[targeted])\n",
    "    y_test = test_set[targeted]\n",
    "    X_test_processed = preprocess_data(X_test)\n",
    "    \n",
    "    def compute_rmsle(y_test: np.ndarray, y_pred: np.ndarray, precision: int = 2) -> float:\n",
    "        rmsle = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return round(rmsle, precision)\n",
    "    \n",
    "    # Make predictions and evaluate the model\n",
    "    y_pred_test = model.predict(X_test_processed)\n",
    "    y_pred_train = model.predict(X_train_processed)\n",
    "    \n",
    "    rmsle_test = compute_rmsle(y_test, y_pred_test)\n",
    "    rmsle_train = compute_rmsle(y_train, y_pred_train)\n",
    "    \n",
    "    print(f'Training RMSLE: {rmsle_train}')\n",
    "    print(f'Testing RMSLE: {rmsle_test}')\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e1c13b-f8e1-49e7-a243-d6331a26293e",
   "metadata": {},
   "source": [
    "# Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5599cba4-4b42-49fb-b692-f57d7b0217a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(input_data: pd.DataFrame) -> np.ndarray:\n",
    "    # Load the model and preprocessors\n",
    "    model = joblib.load(\"../models/model.joblib\")\n",
    "    scaler = joblib.load(\"../models/scaler.joblib\")\n",
    "    one_hot_encoder = joblib.load(\"../models/one_hot_encoder.joblib\")\n",
    "    numeric_imputer = joblib.load(\"../models/numeric_imputer.joblib\")\n",
    "    categorical_imputer = joblib.load(\"../models/categorical_imputer.joblib\")\n",
    "    \n",
    "    # Preprocess inference data\n",
    "    X_inference_processed = preprocess_data(input_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_inference_processed)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8e75ef22-980c-40cc-a237-31fd990f5b8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [292, 1168]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(FILE_PATH)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Build model and evaluate\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model_performance \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Performance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_performance\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Make predictions on new data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[93], line 36\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     33\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_processed)\n\u001b[0;32m     34\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train_processed)\n\u001b[1;32m---> 36\u001b[0m rmsle_test \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_rmsle\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m rmsle_train \u001b[38;5;241m=\u001b[39m compute_rmsle(y_train, y_pred_train)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining RMSLE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmsle_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[93], line 29\u001b[0m, in \u001b[0;36mbuild_model.<locals>.compute_rmsle\u001b[1;34m(y_test, y_pred, precision)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_rmsle\u001b[39m(y_test: np\u001b[38;5;241m.\u001b[39mndarray, y_pred: np\u001b[38;5;241m.\u001b[39mndarray, precision: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m---> 29\u001b[0m     rmsle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mround\u001b[39m(rmsle, precision)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\my-env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\my-env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:506\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    503\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    504\u001b[0m         )\n\u001b[1;32m--> 506\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    510\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\my-env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:111\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m--> 111\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\my-env\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [292, 1168]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    FILE_PATH = \"../data/train.csv\"\n",
    "    dataset = pd.read_csv(FILE_PATH)\n",
    "    \n",
    "    # Build model and evaluate\n",
    "    model_performance = build_model(dataset)\n",
    "    print(f\"Model Performance: {model_performance}\")\n",
    "    \n",
    "    # Make predictions on new data\n",
    "    inference_file_path = \"../data/test.csv\"\n",
    "    inference_data = pd.read_csv(inference_file_path)\n",
    "    predictions = make_predictions(inference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64abed-6714-47af-9c70-5e9093038761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
