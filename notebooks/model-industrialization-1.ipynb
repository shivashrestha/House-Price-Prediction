{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c78a539e-e393-4f68-af45-eff71659d72f",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f51f4-d968-40da-9c56-517b51c92d19",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab1a9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# ************Features Selection************\n",
    "\n",
    "# Continuous features\n",
    "continuous_features = ['LotArea', 'YearBuilt', '1stFlrSF', 'GrLivArea']\n",
    "\n",
    "# Categorical features\n",
    "kitchen_quality_column = 'KitchenQual'\n",
    "categorical_features = ['Neighborhood', 'HouseStyle', 'OverallQual', 'OverallCond', kitchen_quality_column]\n",
    "kitchen_quality_dict = {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0}\n",
    "\n",
    "# Target value\n",
    "targeted = 'SalePrice'\n",
    "\n",
    "# ************Preprocessing*****************\n",
    "\n",
    "\n",
    "def preprocess_data(X, fit=False):\n",
    "\n",
    "    # Initialize the scalers and encoders\n",
    "    scaler = StandardScaler()\n",
    "    one_hot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "    \n",
    "    # Initialize the imputers\n",
    "    numeric_imputer = SimpleImputer(strategy='median')\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    if(fit):\n",
    "        # Fit and transform continuous features for training set\n",
    "        X_train_continuous = X[continuous_features]\n",
    "        numeric_imputer.fit(X_train_continuous)\n",
    "        X_train_continuous = numeric_imputer.transform(X_train_continuous)\n",
    "        scaler.fit(X_train_continuous)\n",
    "        X_train_continuous = scaler.transform(X_train_continuous)\n",
    "        \n",
    "        # Fit and transform categorical features for training set\n",
    "        X_train_categorical = X[categorical_features].copy()\n",
    "        X_train_categorical[kitchen_quality_column] = X_train_categorical[kitchen_quality_column].map(kitchen_quality_dict)\n",
    "        categorical_imputer.fit(X_train_categorical)\n",
    "        X_train_categorical = categorical_imputer.transform(X_train_categorical)\n",
    "        one_hot_encoder.fit(X_train_categorical[:, :-1])\n",
    "        X_train_categorical_encoded = one_hot_encoder.transform(X_train_categorical[:, :-1])\n",
    "        X_train_kitchen_quality = X_train_categorical[:, -1].reshape(-1, 1)\n",
    "        # Combine preprocessed features for training set\n",
    "        X_train_processed = np.hstack((X_train_continuous, X_train_categorical_encoded, X_train_kitchen_quality))\n",
    "        # Save the model, encoders, and scalers\n",
    "        joblib.dump(scaler, \"../models/scaler.joblib\")\n",
    "        joblib.dump(one_hot_encoder, \"../models/one_hot_encoder.joblib\")\n",
    "        joblib.dump(numeric_imputer, \"../models/numeric_imputer.joblib\")\n",
    "        joblib.dump(categorical_imputer, \"../models/categorical_imputer.joblib\")\n",
    "    else:\n",
    "        # Load preprocessors\n",
    "        scaler = joblib.load(\"../models/scaler.joblib\")\n",
    "        one_hot_encoder = joblib.load(\"../models/one_hot_encoder.joblib\")\n",
    "        numeric_imputer = joblib.load(\"../models/numeric_imputer.joblib\")\n",
    "        categorical_imputer = joblib.load(\"../models/categorical_imputer.joblib\")\n",
    "        \n",
    "        # Transform continuous features\n",
    "        X_continuous = X[continuous_features]\n",
    "        X_continuous = numeric_imputer.transform(X_continuous)\n",
    "        X_continuous = scaler.transform(X_continuous)\n",
    "        \n",
    "        # Transform categorical features\n",
    "        X_categorical = X[categorical_features].copy()\n",
    "        X_categorical[kitchen_quality_column] = X_categorical[kitchen_quality_column].map(kitchen_quality_dict)\n",
    "        X_categorical = categorical_imputer.transform(X_categorical)\n",
    "        X_categorical_encoded = one_hot_encoder.transform(X_categorical[:, :-1])\n",
    "        X_kitchen_quality = X_categorical[:, -1].reshape(-1, 1)\n",
    "        # Combine preprocessed features\n",
    "        X_train_processed = np.hstack((X_continuous, X_categorical_encoded, X_kitchen_quality))\n",
    "    \n",
    "    print(X_train_processed)\n",
    "    return  X_train_processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f42f7",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3110b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(data: pd.DataFrame) -> dict[str, str]:\n",
    "    # Split the data into train and test sets\n",
    "    train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Split features and target from the train set\n",
    "    X_train = train_set.drop(columns=[targeted])\n",
    "    y_train = train_set[targeted]\n",
    "        \n",
    "    # Preprocess training data\n",
    "    X_train_processed = preprocess_data(X_train, fit=True)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_processed, y_train)\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(model, \"../models/model.joblib\")\n",
    "\n",
    "    # Split features and target from the test set\n",
    "    X_test = test_set.drop(columns=[targeted])\n",
    "    y_test = test_set[targeted]\n",
    "\n",
    "    # Model evaluation\n",
    "    X_test = test_set.drop(columns=[targeted])\n",
    "    y_test = test_set[targeted]\n",
    "    X_test_processed = preprocess_data(X_test, fit=False)\n",
    "    \n",
    "    def compute_rmsle(y_test: np.ndarray, y_pred: np.ndarray, precision: int = 2) -> float:\n",
    "        rmsle = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return round(rmsle, precision)\n",
    "    \n",
    "    # Make predictions and evaluate the model\n",
    "    y_pred_test = model.predict(X_test_processed)\n",
    "    y_pred_train = model.predict(X_train_processed)\n",
    "    \n",
    "    rmsle_test = compute_rmsle(y_test, y_pred_test)\n",
    "    rmsle_train = compute_rmsle(y_train, y_pred_train)\n",
    "    \n",
    "    print(f'Training RMSLE: {rmsle_train}')\n",
    "    print(f'Testing RMSLE: {rmsle_test}')\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e1c13b-f8e1-49e7-a243-d6331a26293e",
   "metadata": {},
   "source": [
    "# Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5599cba4-4b42-49fb-b692-f57d7b0217a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(input_data: pd.DataFrame) -> np.ndarray:\n",
    "    # Load the model and preprocessors\n",
    "    model = joblib.load(\"../models/model.joblib\")\n",
    "    scaler = joblib.load(\"../models/scaler.joblib\")\n",
    "    one_hot_encoder = joblib.load(\"../models/one_hot_encoder.joblib\")\n",
    "    numeric_imputer = joblib.load(\"../models/numeric_imputer.joblib\")\n",
    "    categorical_imputer = joblib.load(\"../models/categorical_imputer.joblib\")\n",
    "    \n",
    "    # Preprocess inference data\n",
    "    X_inference_processed = preprocess_data(input_data, fit=False)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_inference_processed)\n",
    "    print(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e75ef22-980c-40cc-a237-31fd990f5b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.21289571021689285 -0.4554689634533773 0.3742352301895073 ... 0.0 0.0\n",
      "  2]\n",
      " [-0.26524462694629186 0.7186089531287432 -0.9582022093686505 ... 0.0 0.0\n",
      "  2]\n",
      " [-0.17784146224711767 -1.9882929101022566 -0.9659639808612224 ... 0.0\n",
      "  0.0 2]\n",
      " ...\n",
      " [-0.2340956267609479 -0.5206955143746061 -0.7900304936962618 ... 0.0 0.0\n",
      "  2]\n",
      " [-0.2833761345168653 -1.7273867064173412 -0.665842149815113 ... 1.0 0.0\n",
      "  3]\n",
      " [-0.6513992471544521 1.1751948095773455 0.9977642067594413 ... 0.0 0.0 3]]\n",
      "[[-0.2115939609554158 -0.2597893106896905 -0.26223003220137975 ... 1.0\n",
      "  0.0 2]\n",
      " [0.14564322922993247 0.7512222285893576 0.8554650627289585 ... 0.0 0.0 3]\n",
      " [-0.16082573975781034 -1.433867227271811 -0.3657203187690037 ... 0.0 0.0\n",
      "  2]\n",
      " ...\n",
      " [-0.23158511032809925 1.1099682586561166 -1.141897468026183 ... 0.0 0.0\n",
      "  3]\n",
      " [-0.14929596058472777 -1.009894646283823 -1.0720415245930368 ... 0.0 0.0\n",
      "  2]\n",
      " [-0.2389306954464341 -0.031496382465389355 -0.7900304936962618 ... 0.0\n",
      "  0.0 2]]\n",
      "Training RMSLE: 30824.63\n",
      "Testing RMSLE: 33015.21\n",
      "[[0.08669258410304263 -0.32501586161091944 -0.7072382644421625 ... 0.0\n",
      "  0.0 2.0]\n",
      " [0.332630212432102 -0.4228556879927628 0.4130440876523663 ... 0.0 0.0\n",
      "  3.0]\n",
      " [0.29199703905599655 0.8490620549712009 -0.6244460351880634 ... 0.0 0.0\n",
      "  2.0]\n",
      " ...\n",
      " [0.8656965350069593 -0.35762913707153393 0.1413820854123535 ... 0.0 0.0\n",
      "  2.0]\n",
      " [-0.023119264311558183 0.6859956776681286 -0.5157812342920584 ... 0.0\n",
      "  0.0 2.0]\n",
      " [-0.09880668565743884 0.7186089531287432 -0.4485125480231028 ... 0.0 0.0\n",
      "  2.0]]\n",
      "[124538.22227202 168046.1462684  163723.90367148 ... 154145.43375264\n",
      " 137204.06751157 195544.85962939]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    FILE_PATH = \"../data/train.csv\"\n",
    "    dataset = pd.read_csv(FILE_PATH)\n",
    "    \n",
    "    # Build model and evaluate\n",
    "    model_performance = build_model(dataset)\n",
    "    \n",
    "    # Make predictions on new data\n",
    "    inference_file_path = \"../data/test.csv\"\n",
    "    inference_data = pd.read_csv(inference_file_path)\n",
    "    predictions = make_predictions(inference_data)\n",
    "    predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64abed-6714-47af-9c70-5e9093038761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
